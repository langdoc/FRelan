---
title: "FRelan vignette: read_eaf()"
author: "Niko Partanen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::knitr}
  %\VignetteEncoding{utf8}
---

This vignette explains how to use `read_eaf()` function recursevely for larger number of ELAN files. There is an older version of it called `read_eaf_old()`, and it certainly will be removed from the package in the future. However, it currently does much more useful maintenance work which the new function is not up to.

The package FRelan can be installed from GitHub with the commands:

```{r, eval=F}
devtools::install_github("langdoc/FRelan")
```

The package can be loaded with following commands. I recommend loading `plyr` and `dplyr` at the same time.

```{r, message=F}
library(FRelan)
library(plyr)
library(dplyr)
```

The main function, `read_eaf()`, can be used in a following manner:

```{r}
eaf <- read_eaf("/Volumes/langdoc/langs/kpv/kpv_izva19580000CypanovDM/kpv_izva19580000CypanovDM.eaf")
eaf
```

So it returns a data frame which contains the most of the data that is in this particular ELAN file. However, often reading one ELAN file is not enough, but we want to get access to the whole corpus. The easiest way to do this, in my opinion, is to use `plyr` package to loop over all files. This can be done in a following fashion.

The first step is to find all the files. We can use `list.files()` function to this. We can set it to search ELAN files recursively from the given folder.

```{r}
eaf <- list.files(path = "/Volumes/langdoc/langs/kpv/", pattern = "eaf$", recursive = T, full.names = T)
eaf <- tail(eaf)
```

This character vector contains `r length(eaf)` elements, which are all Komi-Zyrian ELAN files we have in the corpus at the moment. They are arranged alphabetically, which results in dialect/year based ordering. However, now we can use some functions from `plyr` to parse them all. We also use `tbl_df` function from `dplyr` to turn it into a `local data frame`, which is often nicer to examine than the ordinary data frame.

```{r}
corpus_kpv <- ldply(eaf, read_eaf) %>% tbl_df
```

Now we have an object `corpus_kpv` which contains almost everything that should be in the corpus. It is quite long, `r nrow(corpus_kpv)` rows, and each row is one token. Two files were skipped for some structural problems. Often these are new files which have something missing, i.e. tokenization is not done or some tiers are just empty. However, now this function usually manages to parse the files and just gives a message when it cannot go forward. The result looks like this.

```{r}
corpus_kpv
```

There are now lots of operations we can do with this. Many basic searches are very easy to do with different filtering operations, but I have simplified some of those into this package as well. As an example:

```{r}

```

